general:
  task: "rl_planning"  #
  random_seed: 42
  use_this_many_data: -1
  use_cuda: True  # disable this when running on machine without cuda
  visdom: False

  training:
    batch_size: 50
    max_episode: 500000
    smoothing_eps: 0.1
    sample_number: 10
    optimizer:
      step_rule: 'radam'  # adam, radam
      learning_rate: 0.0001
      clip_grad_norm: 5
      learning_rate_warmup_until: 1000
    fix_parameters_keywords: ["rgcns", "word_embedding", "node_embedding", "relation_embedding", "word_embedding_prj", "q_target_net"]
    patience: 3  # >=1 to enable

  evaluate:
    run_eval: True
    g_belief: False
    batch_size: 20
    max_target_length: 200

  model:
    use_pretrained_embedding: True
    word_embedding_size: 300
    word_embedding_trainable: False
    node_embedding_size: 100
    node_embedding_trainable: True
    relation_embedding_size: 32
    relation_embedding_trainable: True
    embedding_dropout: 0.
    encoder_layers: 1
    action_scorer_layers: 1
    encoder_conv_num: 5
    block_hidden_dim: 32
    gcn_hidden_dims: [32, 32, 32, 32, 32, 32]  # last one should equal to block hidden dim
    gcn_highway_connections: True
    gcn_num_bases: 3
    real_valued_graph: True
    n_heads: 1
    dropout: 0.
    attention_dropout: 0.
    block_dropout: 0.
    topk_num: 10
    graph_decoding_method: 'ComplEx' # DistMult, ComplEx
    dynamic_model_type: 'linear'
    dynamic_model_mechanism: 'all-independent'
    dynamic_loss_type: 'latent'
    reward_predictor_apply_rnn: False

  checkpoint:
    output_dir: '../saved_models/'
    report_frequency: 1000  # episode
    save_frequency: 10000  # episode
    experiment_tag: 'rl_plannning'
    load_pretrained: True
    load_from_tag: ['graph_auto_encoder_pretrain', 'graph_dynamics_ims', 'reward_predictor']
    load_from_label: ['Mar-04-2021', '', '']
    data_label: ''
    fix_loaded_parameters: False
    load_partial_parameter_keywords: [] # "rgcns", "word_embedding", "node_embedding", "relation_embedding", "word_embedding_prj"
    load_graph_generation_model_from_tag: 'None'

rl:
  data_path: "rl.0.2"
  triplet_path: "../source/dataset/sp.0.2"
  difficulty_level: 3  # 1-10
  training_size: 100  # 1, 20, 100
  graph_type: "seen"  # full, seen
  max_scores: 4

  planing:
    planner: 'MCTS'  # CEM, MCTS
    c_puct: 0.1
    discount_rate: 0.9
    max_search_depth: 'inf'
    random_move_prob_add: 0
    simulations_num: 300
    load_extractor: False

  training:
    max_nb_steps_per_episode: 200  # after this many steps, a game is terminated
    learn_start_from_this_episode: 100
    target_net_update_frequency: 500  # sync target net with online net per this many epochs
    use_negative_reward: False
    apply_goal_constraint: True

  replay:
    buffer_reward_threshold: 0.1  # a new sequence of transitions has to be k times better than average rewards in current replay buffer. Use -1 to disable.
    accumulate_reward_from_final: False
    prioritized_replay_beta: 0.4
    prioritized_replay_eps: 0.000001
    count_reward_lambda: 0.0  # 0 to disable
    graph_reward_lambda: 0.0  # 0 to disable
    graph_reward_type: "triplets_difference"  # triplets_difference, triplets_increased
    discount_gamma_graph_reward: 0.9
    discount_gamma_count_reward: 0.5
    discount_gamma_game_reward: 0.9
    replay_memory_capacity: 300000  # adjust this depending on your RAM size
    replay_memory_priority_fraction: 0.6
    update_per_k_game_steps: 20
    replay_batch_size: 64
    multi_step: 1
    replay_sample_history_length: 8
    replay_sample_update_from: 4

  epsilon_greedy:
    noisy_net: False  # if this is true, then epsilon greedy is disabled
    epsilon_anneal_episodes: 20000  # -1 if not annealing
    epsilon_anneal_from: 1.0
    epsilon_anneal_to: 0.3
    min_unexplore_rate: 0.3

  evaluate:
    max_nb_steps_per_episode: 200
